{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ee6600",
   "metadata": {},
   "source": [
    "# <center>Welcome to my DSCI 230 Final Project!</center>\n",
    "**We will be exploring some data supplied by a mountain biking company in Bentonville, AR. \n",
    "The company has customers who register before walking in, and customers who come without a reservation. We call these *registered* and *casual* customers. Here are some things we want to know:**\n",
    "- If weather affects one group of customers more than another\n",
    "- If there are thresholds in which a group may decide not to come (snow, rain, high temps, etc.)\n",
    "- Explore a relationship between weather attributes and registered/casual customers\n",
    "\n",
    "**We will use linear regression to do this. We'll start by loading in the libraries we need for this project:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3790b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fba67",
   "metadata": {},
   "source": [
    "**Now we will load our dataset and begin cleaning it. This includes finding where we have missing values, then filling those in. We also want to find any outliers in the data, and go ahead and remove those. Here, I create two helper functions to achieve this. Lastly, we remove columns we don't need, these include things like latitude, longitude, and elevation, which are values that don't change since all of this data was curated in the same location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0b5166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of unfiltered data: (355, 13)\n",
      "\n",
      "Missing value counts:\n",
      " STATION            0\n",
      "DATE               0\n",
      "LATITUDE           0\n",
      "LONGITUDE          0\n",
      "ELEVATION          0\n",
      "NAME               0\n",
      "PRCP_MM            0\n",
      "TMAX_10THC         3\n",
      "TMIN_10THC         1\n",
      "AWND_MTRSPERSEC    0\n",
      "CASUAL             0\n",
      "REGISTERED         1\n",
      "TTLCNT             1\n",
      "dtype: int64\n",
      "\n",
      "Missing value counts after cleaning:\n",
      " STATION            0\n",
      "DATE               0\n",
      "LATITUDE           0\n",
      "LONGITUDE          0\n",
      "ELEVATION          0\n",
      "NAME               0\n",
      "PRCP_MM            0\n",
      "TMAX_10THC         0\n",
      "TMIN_10THC         0\n",
      "AWND_MTRSPERSEC    0\n",
      "CASUAL             0\n",
      "REGISTERED         0\n",
      "TTLCNT             0\n",
      "dtype: int64\n",
      "\n",
      "Shape of filtered data: (313, 8)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"ARBike.csv\")\n",
    "print(\"Shape of unfiltered data:\", data.shape)\n",
    "print(\"\\nMissing value counts:\\n\",data.isna().sum())   # We can see there are just a few datapoints missing that we have to take care of\n",
    "\n",
    "# This function will take a dataset and column name or list of names as input and fill NA values with the median\n",
    "def fill_with_median(data, column_name):\n",
    "    if isinstance(column_name, list):\n",
    "        for col in column_name:\n",
    "            impute_val = data[col].median()\n",
    "            data[col] = data[col].fillna(impute_val)\n",
    "    else:\n",
    "        impute_val = data[column_name].median()\n",
    "        data[column_name] = data[column_name].fillna(impute_val)\n",
    "\n",
    "data.columns = data.columns.str.strip()  # Some columns had unnecessary extra whitespace characters\n",
    "cols_with_NA = [\"TMAX_10THC\", \"TMIN_10THC\", \"REGISTERED\", \"TTLCNT\"]  \n",
    "fill_with_median(data, cols_with_NA)\n",
    "print(\"\\nMissing value counts after cleaning:\\n\",data.isna().sum())   # Now we can see all the NA values are gone\n",
    "\n",
    "# This function will take a dataset and a column name and filter outliers using IQR (interquartile range)\n",
    "def remove_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    iqr = Q3 - Q1\n",
    "    lower_bound = Q1 - 10 * iqr\n",
    "    upper_bound = Q3 + 10 * iqr\n",
    "    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    return filtered_data\n",
    "\n",
    "outlier_potentials = [\"PRCP_MM\", \"TMAX_10THC\", \"TMIN_10THC\", \"AWND_MTRSPERSEC\", \"REGISTERED\", \"CASUAL\", \"TTLCNT\"]\n",
    "for col in outlier_potentials:\n",
    "    data = remove_outliers_iqr(data, col)\n",
    "    \n",
    "# We will use indexing options to create a new dataset that has only the columns we need\n",
    "# Columns like station, latitude, longitude, elevation, and name are not needed because \n",
    "# they're the same throughout the entire dataset\n",
    "data = data[['DATE', 'PRCP_MM', 'TMAX_10THC', 'TMIN_10THC', 'AWND_MTRSPERSEC', 'CASUAL', 'REGISTERED', 'TTLCNT']]\n",
    "data.head()\n",
    "\n",
    "print(\"\\nShape of filtered data:\", data.shape)  # Our function detected 73 outliers, but now we know our data is clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5f0a3",
   "metadata": {},
   "source": [
    "**Next, we'll perform some operations on the data to familiarize ourselves with the dataset. In this cell, we're going to analyze a couple different things. These include:**\n",
    "- Sorting the dataset by registered users to see what conditions brought the most registered customers\n",
    "- Computing a boolean array to see which rows have a temperature of at least 200\n",
    "- Using join operations to merge two subsets of the data together\n",
    "- Using GroupBy methods to view total viewers by month and average users per weekday\n",
    "- Slice the dataset to look at a certain month with DateTime\n",
    "- Create a Period Index for quarterly analysis\n",
    "- Find the distance between a date in the highest attended month vs the lowest attended month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1eb1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          DATE  PRCP_MM  TMAX_10THC  TMIN_10THC  AWND_MTRSPERSEC  CASUAL  \\\n",
      "234  8/29/2021        0       300.0       217.0               24    1281   \n",
      "180   7/6/2021        0       306.0       189.0               16    1027   \n",
      "241   9/5/2021        0       289.0       194.0               30     775   \n",
      "179   7/5/2021        0       289.0       183.0               22     848   \n",
      "272  10/7/2021        0       261.0       133.0               13     830   \n",
      "\n",
      "     REGISTERED  TTLCNT  \n",
      "234      4614.0  5895.0  \n",
      "180      4488.0  5515.0  \n",
      "241      4429.0  5204.0  \n",
      "179      4377.0  5225.0  \n",
      "272      4372.0  5202.0  \n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "7    False\n",
      "Name: TMAX_10THC, dtype: bool\n",
      "\n",
      "         DATE  CASUAL  TMAX_10THC  REGISTERED\n",
      "0   1/2/2021     131        11.0       670.0\n",
      "1   1/4/2021     108       122.0      1454.0\n",
      "2   1/8/2021      68        28.0       891.0\n",
      "3  1/10/2021      41        17.0      1280.0\n",
      "4  1/12/2021      25        89.0      1137.0\n",
      "       DATE  CASUAL  TMAX_10THC  REGISTERED\n",
      "0  1/2/2021   131.0        11.0       670.0\n",
      "1  1/3/2021     NaN         NaN      1229.0\n",
      "2  1/4/2021   108.0       122.0      1454.0\n",
      "3  1/5/2021     NaN         NaN      1518.0\n",
      "4  1/8/2021    68.0        28.0       891.0\n",
      "\n",
      "Total Number of Customers by Month:\n",
      "      REGISTERED  CASUAL\n",
      "DATE                    \n",
      "1        28324.0    2278\n",
      "2        37473.0    4965\n",
      "3        42069.0    9945\n",
      "4        56906.0   17645\n",
      "5        88811.0   22369\n",
      "6        88721.0   28045\n",
      "7       101433.0   35285\n",
      "8       105352.0   28807\n",
      "9        80349.0   22367\n",
      "10       92211.0   25431\n",
      "11       67770.0   12117\n",
      "12       69398.0    8046\n",
      "\n",
      "Average Customers per Weekday:\n",
      "       REGISTERED      CASUAL\n",
      "DATE                         \n",
      "0     2767.282609  559.760870\n",
      "1     2736.454545  584.250000\n",
      "2     3009.700000  675.875000\n",
      "3     2615.500000  894.750000\n",
      "4     2496.738095  921.928571\n",
      "5     2610.872340  675.361702\n",
      "6     2961.480000  579.540000\n",
      "\n",
      "Customer Attendance in August:\n",
      "          DATE  PRCP_MM  TMAX_10THC  TMIN_10THC  CASUAL  REGISTERED\n",
      "206 2021-08-01        8       283.0       211.0     750      3840.0\n",
      "207 2021-08-02        0       294.0       189.0     755      3901.0\n",
      "208 2021-08-03        0       289.0       183.0     606      3784.0\n",
      "209 2021-08-04        0       289.0       189.0     670      3176.0\n",
      "210 2021-08-05        0       272.0       194.0    1559      2916.0\n",
      "\n",
      "Quarterly Analysis:\n",
      "        DATE  TMAX_10THC  TMIN_10THC  CASUAL  REGISTERED QUARTER\n",
      "1 2021-01-02        11.0       -10.0     131       670.0  2021Q1\n",
      "2 2021-01-03       133.0       -32.0     120      1229.0  2021Q1\n",
      "3 2021-01-04       122.0       -27.0     108      1454.0  2021Q1\n",
      "4 2021-01-05       144.0       -38.0      82      1518.0  2021Q1\n",
      "7 2021-01-08        28.0       -10.0      68       891.0  2021Q1\n",
      "\n",
      "Duration between 2021-08-21 00:00:00 and 2021-01-03 00:00:00 : 230 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# We can sort the dataset by registered users to see the what conditions brought the most registered users\n",
    "highest_reg = data.sort_values(by='REGISTERED', ascending=False)\n",
    "print(highest_reg.head())\n",
    "\n",
    "# We can compute a boolean array to see which rows have a temp of at least 200\n",
    "temp_bool_arr = data['TMAX_10THC'] >= 200\n",
    "temp_bool_arr[temp_bool_arr == False].count()\n",
    "print(temp_bool_arr.head())\n",
    "\n",
    "# We can also demonstrate two different types of joins by creating two datasets and merging them together\n",
    "df1 = data[['DATE','CASUAL', 'TMAX_10THC']].iloc[::2]\n",
    "df2 = data[['DATE', 'REGISTERED']]\n",
    "\n",
    "inner_join = pd.merge(df1, df2, on='DATE', how='left')\n",
    "outer_join = pd.merge(df1, df2, on='DATE', how='right')\n",
    "print(\"\\n\", inner_join.head())\n",
    "print(outer_join.head())\n",
    "\n",
    "# We can use 2 different groupby methods to view total number of users by month and average users per weekday\n",
    "# First, convert 'DATE' column to datetime format\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "# Total number of customers by month\n",
    "monthly_users = data.groupby(data['DATE'].dt.month)[['REGISTERED', 'CASUAL']].sum()\n",
    "print(\"\\nTotal Number of Customers by Month:\")\n",
    "print(monthly_users)\n",
    "\n",
    "# Average customers per weekday\n",
    "weekday_users = data.groupby(data['DATE'].dt.weekday)[['REGISTERED', 'CASUAL']].mean()\n",
    "print(\"\\nAverage Customers per Weekday:\")\n",
    "print(weekday_users)\n",
    "\n",
    "# We can slice the dataset to focus on a specific month with datetime\n",
    "# We will look at August, since we saw from before it brought the highest registered customer attendance\n",
    "august_data = data[data['DATE'].dt.month == 8]\n",
    "print(\"\\nCustomer Attendance in August:\")\n",
    "print(august_data[['DATE', 'PRCP_MM', 'TMAX_10THC', 'TMIN_10THC', 'CASUAL', 'REGISTERED']].head())\n",
    "\n",
    "# We can create a period index for quarterly analysis\n",
    "quarterly_index = pd.PeriodIndex(data['DATE'], freq='Q')\n",
    "data['QUARTER'] = quarterly_index\n",
    "print(\"\\nQuarterly Analysis:\")\n",
    "print(data[['DATE', 'TMAX_10THC', 'TMIN_10THC', 'CASUAL', 'REGISTERED', 'QUARTER']].head())\n",
    "\n",
    "# We can calculate the duration between two specific dates\n",
    "# We will calculate the distance between a random date in the highest attended month and the lowest\n",
    "date1 = pd.to_datetime('2021-08-21')\n",
    "date2 = pd.to_datetime('2021-01-03')\n",
    "duration = date1 - date2\n",
    "print(\"\\nDuration between\", date1, \"and\", date2, \":\", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f8922",
   "metadata": {},
   "source": [
    "**Next up, we'll create some visualizations. This includes line plots, vertical and horizontal plots, scatter plots, and box plots. This will help us draw conclusions and make inferences on our data. The first one we'll make is a line plot that will look at the number of casuals and registered customers over the year:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2f47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by splitting our data into 2 sets: casual and registered\n",
    "casual = data[['DATE', 'PRCP_MM', 'TMAX_10THC', 'TMIN_10THC', 'AWND_MTRSPERSEC', 'CASUAL']]\n",
    "registered = data[['DATE', 'PRCP_MM', 'TMAX_10THC', 'TMIN_10THC', 'AWND_MTRSPERSEC', 'REGISTERED']]\n",
    "\n",
    "# Now we will create a line plot with point markers for registered customers\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot registered customers\n",
    "plt.plot(registered['DATE'], registered['REGISTERED'], marker='o', linestyle='-', color='green', label='Registered Customers')\n",
    "# Plot casual customers\n",
    "plt.plot(casual['DATE'], casual['CASUAL'], marker='o', linestyle='-', color='darkblue', label='Casual Customers')\n",
    "# Add details like labels and a legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Registered and Casual Customer Counts over the Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(registered['DATE'][::15])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb9269",
   "metadata": {},
   "source": [
    "**We can see from this plot that the company gets a significantly larger amount of registered customers than casual customers. This is pretty expected for a destination regarded as having 'world-class mountain biking trails'. We can also see that their peak number of customers comes in the summer months. This is also pretty expected due to warmer weather and more people traveling/on vacation. So from this, we can pretty safely infer that the time of year doesn't trend to more registered customers visiting compared to casual, or vice versa.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcada23",
   "metadata": {},
   "source": [
    "**Next, we'll look at two different bar plots. One will view the number of customers by wind speed, and the other will view the number of customers by the amount of precipitation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da04d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical Bar Plot: Number of customers by wind speed\n",
    "wind_users = data.groupby('AWND_MTRSPERSEC')[['REGISTERED', 'CASUAL']].sum()\n",
    "wind_users.plot(kind='bar', figsize=(10, 6), color=['red', 'blue'])\n",
    "plt.title('Number of Customers in Relation to Wind Speeds')\n",
    "plt.xlabel('Wind Speed (meters per second)')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Registered', 'Casual'])\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Horizontal Bar Plot: Number of customers by amount of rain/snow\n",
    "weather_users = data.groupby('PRCP_MM')[['REGISTERED', 'CASUAL']].sum()\n",
    "weather_users.plot(kind='barh', figsize=(10, 6), color=['skyblue', 'purple'])\n",
    "plt.title('Number of Customers in Relation to Amount of Precipitation')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Milimeters of Rain/Snow')\n",
    "plt.legend(['Registered', 'Casual'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ef2a1",
   "metadata": {},
   "source": [
    "**There are some obvious observations for these plots as well. We can see that attendance drops off a cliff when there is essentially any rain. This makes sense because let's be honest, who wants to hike or go on bike trails in the rain? The other plot, however, shows a huge spike in attendance at a wind speed of 30 meters per second. This is likely either a coincidence or the data just has a lot of samples with a wind speed of 30 m/s. Outside of that, everything looks pretty standard. To me, it doesn't look like wind speed has a huge correlation with attendance, I think it's just moreso that we have a lot more data samples in the 15-40 m/s range than we do anything more or less than that. From this, we can infer that neither wind speeds or precipitation amounts will lead to more/less attendance from one group of customers compared to the other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7dcb6",
   "metadata": {},
   "source": [
    "**Next we'll view a scatter plot comparing the high temperature for a given day and the number of total customers that came to the mountain biking company that day. We will also fit a regression line on this plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05dffff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot with regression lines for REGISTERED customers\n",
    "sns.regplot(x='TMAX_10THC', y='REGISTERED', data=data, scatter_kws={'color': 'blue'}, line_kws={'color': 'blue'})\n",
    "# Scatter Plot with regression lines for CASUAL customers\n",
    "sns.regplot(x='TMAX_10THC', y='CASUAL', data=data, scatter_kws={'color': 'orange'}, line_kws={'color': 'orange'})\n",
    "\n",
    "plt.title('Number of Total Customers in Relation to Increasing Temperatures')\n",
    "plt.xlabel('Maximum Temperature')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# The legend wouldn't work right, so I had to jump through some hoops for it\n",
    "legend_handles = [plt.Line2D([0], [0], color='blue', marker='o', linestyle='None', label='REGISTERED'),\n",
    "                  plt.Line2D([0], [0], color='orange', marker='s', linestyle='None', label='CASUAL')]\n",
    "plt.legend(handles=legend_handles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00d148",
   "metadata": {},
   "source": [
    "**We can see from the plot that there's a decent correlation between the number of customers and the temperature. This supports our findings in our previous plot that showed higher attendance during summer months. There doesn't seem to be anything out of the ordinary here either. Both groups seem to go biking more in warmer weather, regardless of if it's a *registered* or *casual* customer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2fdaf5",
   "metadata": {},
   "source": [
    "**Now we're gonna look at a box plot. This will categorize a continuous variable. For this example, we're going to split the dates into the four seasons, and then view customer attendance in relation to the season.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9770e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the month from the 'DATE' column\n",
    "data['MONTH'] = data['DATE'].dt.month\n",
    "\n",
    "# Then we define a helper function to group by season\n",
    "def get_season(month):\n",
    "    if month in [12,1,2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3,4,5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6,7,8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9,10,11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        raise Exception('Error, month data not within required range')\n",
    "\n",
    "# Now we apply the function to create a 'SEASON' column\n",
    "data['SEASON'] = data['MONTH'].apply(get_season)\n",
    "\n",
    "# Finally, we create the box plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='SEASON', y='TTLCNT', data=data)\n",
    "plt.title('Number of Total Customers by Season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Number of Total Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ef788",
   "metadata": {},
   "source": [
    "**There's some interesting observations to be made here. First, I noticed the median line in the winter box isn't close to the middle of the box, meaning we've probably got some skewed data there. We can also see that the box plot for the summer box is pretty small, meaning the data is clustered around the median. Looking at the big picture, summer and fall months attract the most customers, with spring not far behind, while winter is firmly in last. It's also worth noting that we've got some outliers in the summer and fall boxes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e671611c",
   "metadata": {},
   "source": [
    "**Next, we're going to try and create a model that will look at weather features for each day, and try to predict if someone will go biking that day. To start, we'll make a scatter plot with a linear regression line showing the difference between *registered* and *casual* customers' attendance in regards to the low temps, since it's the only weather attribute of the four in the dataset that we haven't looked at yet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9d1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot with regression lines for REGISTERED customers\n",
    "sns.regplot(x='TMIN_10THC', y='REGISTERED', data=data, scatter_kws={'color': 'darkblue'}, line_kws={'color': 'darkblue'})\n",
    "# Scatter Plot with regression lines for CASUAL customers\n",
    "sns.regplot(x='TMIN_10THC', y='CASUAL', data=data, scatter_kws={'color': 'lightblue'}, line_kws={'color': 'lightblue'})\n",
    "\n",
    "plt.title('Number of Total Customers in Relation to Low Temperatures')\n",
    "plt.xlabel('Minimum Temperature')\n",
    "plt.ylabel('Number of Customers')\n",
    "legend_handles = [plt.Line2D([0], [0], color='darkblue', marker='o', linestyle='None', label='REGISTERED'),\n",
    "                  plt.Line2D([0], [0], color='lightblue', marker='s', linestyle='None', label='CASUAL')]\n",
    "plt.legend(handles=legend_handles)\n",
    "\n",
    "# Reverse x-axis to go from high temps to low temps instead of low to high\n",
    "plt.xlim(plt.xlim()[::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1bdac2",
   "metadata": {},
   "source": [
    "**We can clearly see just like in the scatterplot that showed higher attendance for warm days, we see a big drop in attendance when the temperature is cool. Next, we're gonna work on developing the model. We will first put our 'predictors' in an array:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2266d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'predictors' in X array\n",
    "X = data[['PRCP_MM', 'TMAX_10THC', 'TMIN_10THC', 'AWND_MTRSPERSEC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f53f4",
   "metadata": {},
   "source": [
    "**Now, here's the tricky part. We don't have access to data saying whether a customer came or did not come on any given day. So we will have to curate this data ourselves and get a little creative with it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b80dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we calculate mean and standard deviation of each predictor\n",
    "mean_prcp = data['PRCP_MM'].mean()\n",
    "std_prcp = data['PRCP_MM'].std()\n",
    "mean_maxtemp = data['TMAX_10THC'].mean()\n",
    "std_maxtemp = data['TMAX_10THC'].std()\n",
    "mean_mintemp = data['TMIN_10THC'].mean()\n",
    "std_mintemp = data['TMIN_10THC'].std()\n",
    "mean_wind_speed = data['AWND_MTRSPERSEC'].mean()\n",
    "std_wind_speed = data['AWND_MTRSPERSEC'].std()\n",
    "\n",
    "# Now define a function to determine if weather attribute is close enough to the mean\n",
    "def is_close_enough(value, mean_attribute, std_attribute):\n",
    "    # Threshold sets data to where it has to be within one standard deviation\n",
    "    threshold = 1 * std_attribute\n",
    "    return abs(value - mean_attribute) <= threshold\n",
    "\n",
    "def will_bike(row):\n",
    "    # Now we use that function to generate boolean arrays for each attribute\n",
    "    prcp_close = is_close_enough(row['PRCP_MM'], mean_prcp, std_prcp)\n",
    "    maxtemp_close = is_close_enough(row['TMAX_10THC'], mean_maxtemp, std_maxtemp)\n",
    "    mintemp_close = is_close_enough(row['TMIN_10THC'], mean_mintemp, std_mintemp)\n",
    "    wind_close = is_close_enough(row['AWND_MTRSPERSEC'], mean_wind_speed, std_wind_speed)\n",
    "    \n",
    "    if prcp_close and maxtemp_close and mintemp_close and wind_close:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Finally, apply the function and create a new column    \n",
    "data['WILL_BIKE'] = data.apply(will_bike, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521af789",
   "metadata": {},
   "source": [
    "**The method I used to create our 'WILL_BIKE' column is fairly simple. We could have gotten *way* more complex with it, but I figured this was all that was necessary for this project. Now that we have our 'y' column, we can continue with developing the model. What we'll do next is split our data into train and test sets, then use logistic regression to fit the model, and ultimately predict whether or not a potential customer will bike on a given day.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8353f268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.746031746031746\n",
      "Coefficients: [[-0.2544129   0.00728958 -0.00686684  0.00876838]]\n",
      "Intercept: [-1.6636494]\n",
      "R-squared: -0.24444444444444513\n"
     ]
    }
   ],
   "source": [
    "# Now use our new column as our 'y'\n",
    "y = data['WILL_BIKE']\n",
    "\n",
    "# We use train_test_split function to split data into training and testing sets (80% train, 20% test)\n",
    "# This is from sk_learn library, handy for when you don't have separate train and test datasets already\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now instantiate the logistic regression model\n",
    "model = LogisticRegression(C=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Score the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R-squared:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b059d",
   "metadata": {},
   "source": [
    "**And finally, we can see that our model graded out with a 74.6% accuracy. There are also other summary statistics displayed such as coefficients, y-intercept value, and the R-squared value. This means that our model predicts whether or not a potential customer will bike that day with 74.6% accuracy. Not great, but not terrible either. The bike store owner could apply this model to maybe adjust prices or adjust schedules on a given work week, depending on what the weather looks like.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97447900",
   "metadata": {},
   "source": [
    "**That concludes my data analysis project. Please refer to the word doc for a write-up of my findings. This project was pretty fun to work with and I hope you enjoyed learning along with me!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
